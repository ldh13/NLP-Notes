{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Introduction to BERT***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-directional Encoder Representation from Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an auto-encoding language model, uses only the encoder from the transformer, relies on self-attention and the encoder is taken from the transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sentence:\n",
    "\n",
    "\"I love my pet python\"\n",
    "\n",
    "We feed this sentence into BERT to get a context-full representation (vector embedding) of every word in the sentence\n",
    "\n",
    "The encoder understands the context of each word in the sentence using a multi-headed attention mechanism (which relates each word to every other word in the sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nico-\\NLPs\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the model parameters as a list of tuples\n",
    "\n",
    "named_params = list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 199 different named parameters.\n",
      "\n",
      "==== Embedding layer ====\n",
      "\n",
      "embeddings.word_embeddings.weight                       (30522, 768)\n",
      "embeddings.position_embeddings.weight                     (512, 768)\n",
      "embeddings.token_type_embeddings.weight                     (2, 768)\n",
      "embeddings.LayerNorm.weight                                   (768,)\n",
      "embeddings.LayerNorm.bias                                     (768,)\n",
      "encoder.layer.0.attention.self.query.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.query.bias                     (768,)\n",
      "encoder.layer.0.attention.self.key.weight                 (768, 768)\n",
      "encoder.layer.0.attention.self.key.bias                       (768,)\n",
      "encoder.layer.0.attention.self.value.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.value.bias                     (768,)\n",
      "encoder.layer.0.attention.output.dense.weight             (768, 768)\n",
      "encoder.layer.0.attention.output.dense.bias                   (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.weight             (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.bias               (768,)\n",
      "encoder.layer.0.intermediate.dense.weight                (3072, 768)\n",
      "encoder.layer.0.intermediate.dense.bias                      (3072,)\n",
      "encoder.layer.0.output.dense.weight                      (768, 3072)\n",
      "encoder.layer.0.output.dense.bias                             (768,)\n",
      "encoder.layer.0.output.LayerNorm.weight                       (768,)\n",
      "encoder.layer.0.output.LayerNorm.bias                         (768,)\n",
      "\n",
      "==== Output layer ====\n",
      "\n",
      "pooler.dense.weight                                       (768, 768)\n",
      "pooler.dense.bias                                             (768,)\n"
     ]
    }
   ],
   "source": [
    "print(f'The BERT model has {len(named_params)} different named parameters.\\n')\n",
    "\n",
    "print('==== Embedding layer ====\\n')\n",
    "\n",
    "for p in named_params[:5]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "for p in named_params[5:21]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output layer ====\\n')\n",
    "\n",
    "for p in named_params[-2:]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see BERT is aware of 30522 tokens that he can use for any NLP task, this includes the CLS and SEP token.\n",
    "\n",
    "The 768 shows that each of those tokens has a pre-defined context-less embedding of 768 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 8254, 2319, 7459, 1037, 3376, 2154, 102]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enconding a simple sequence\n",
    "\n",
    "tokenizer.encode('Sinan loves a beautiful day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tokens through the model\n",
    "# 1st Turn tokens of unkown words into a tensor (8,)\n",
    "# 2nd Unsqueeze a first dimension to simulate batches (1, 8)\n",
    "\n",
    "response = model(torch.tensor(tokenizer.encode('Sinan loves a beautiful day')).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer will create a list which we then pass to torch to create a tensor and then unsqueeze it on the 0th dimension before passing it to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2327,  0.1515, -0.0448,  ..., -0.5192,  0.4195,  0.2948],\n",
       "         [ 0.3051, -0.6614,  0.2500,  ..., -0.9809,  0.2551,  0.2400],\n",
       "         [-0.3610, -0.8759,  0.4542,  ..., -1.1120,  0.1791,  0.0664],\n",
       "         ...,\n",
       "         [ 0.0689, -0.0364,  0.4940,  ..., -0.6558,  0.2227, -0.3868],\n",
       "         [-0.2657, -0.4257,  0.0056,  ...,  0.1352,  0.3596, -0.4585],\n",
       "         [ 0.6100,  0.0263, -0.2532,  ..., -0.0680, -0.3901, -0.3541]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding for each token\n",
    "\n",
    "response.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each one of these rows represents a token in our sequence, and each column represents that token's context within the greater sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.2327,  0.1515, -0.0448,  ..., -0.5192,  0.4195,  0.2948],\n",
       "         [ 0.3051, -0.6614,  0.2500,  ..., -0.9809,  0.2551,  0.2400],\n",
       "         [-0.3610, -0.8759,  0.4542,  ..., -1.1120,  0.1791,  0.0664],\n",
       "         ...,\n",
       "         [ 0.0689, -0.0364,  0.4940,  ..., -0.6558,  0.2227, -0.3868],\n",
       "         [-0.2657, -0.4257,  0.0056,  ...,  0.1352,  0.3596, -0.4585],\n",
       "         [ 0.6100,  0.0263, -0.2532,  ..., -0.0680, -0.3901, -0.3541]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.8777, -0.4542, -0.6287,  0.7511,  0.3151, -0.0913,  0.9175,  0.3766,\n",
       "         -0.3059, -1.0000, -0.0577,  0.7535,  0.9913,  0.2113,  0.9418, -0.5328,\n",
       "         -0.0568, -0.5698,  0.4090, -0.6096,  0.7876,  0.9995,  0.3670,  0.2453,\n",
       "          0.4620,  0.9465, -0.6802,  0.9342,  0.9614,  0.7060, -0.5755,  0.2076,\n",
       "         -0.9910, -0.1697, -0.8019, -0.9952,  0.3786, -0.7309, -0.0599, -0.0186,\n",
       "         -0.8722,  0.3377,  0.9999, -0.0416,  0.3039, -0.2458, -1.0000,  0.2803,\n",
       "         -0.8856,  0.5071,  0.5182,  0.3099,  0.0669,  0.5149,  0.4535,  0.2464,\n",
       "         -0.0333,  0.1802, -0.2397, -0.5966, -0.5891,  0.4319, -0.6395, -0.9313,\n",
       "          0.4686,  0.1766, -0.1505, -0.2957, -0.0514, -0.1576,  0.8782,  0.2825,\n",
       "          0.1316, -0.8860,  0.1212,  0.2367, -0.4869,  1.0000, -0.3583, -0.9821,\n",
       "          0.5884,  0.2668,  0.4513,  0.3816, -0.0514, -1.0000,  0.3832, -0.1374,\n",
       "         -0.9907,  0.1855,  0.6004, -0.1374,  0.3874,  0.4813, -0.3741, -0.4017,\n",
       "         -0.3040, -0.5284, -0.3583, -0.2307,  0.0355, -0.3318, -0.2008, -0.3483,\n",
       "          0.2099, -0.4482, -0.6539,  0.3030, -0.2450,  0.7254,  0.2816, -0.3301,\n",
       "          0.4079, -0.9599,  0.6782, -0.3683, -0.9862, -0.4576, -0.9919,  0.7549,\n",
       "         -0.2049, -0.2972,  0.9678, -0.0692,  0.2751, -0.1389, -0.5539, -1.0000,\n",
       "         -0.5388, -0.3930,  0.1323, -0.2860, -0.9817, -0.9473,  0.6096,  0.9473,\n",
       "          0.1899,  0.9998, -0.4162,  0.9608,  0.0330, -0.2275,  0.3574, -0.4356,\n",
       "          0.6688,  0.2626, -0.6718,  0.1865, -0.1303,  0.4140, -0.4800, -0.3361,\n",
       "         -0.3105, -0.9495, -0.3550,  0.9570, -0.3836, -0.4769,  0.5362, -0.2674,\n",
       "         -0.5366,  0.8518,  0.3751,  0.4420, -0.1930,  0.4464,  0.1023,  0.5376,\n",
       "         -0.8681,  0.1531,  0.3970, -0.2881, -0.5797, -0.9841, -0.3196,  0.4932,\n",
       "          0.9899,  0.8004,  0.2937,  0.5190, -0.2158,  0.4647, -0.9605,  0.9846,\n",
       "         -0.2993,  0.3152, -0.3622,  0.2491, -0.8298, -0.0492,  0.8649, -0.5621,\n",
       "         -0.8109,  0.0596, -0.4256, -0.4160, -0.6402,  0.4875, -0.3528, -0.4344,\n",
       "         -0.1472,  0.9358,  0.9607,  0.8050, -0.2240,  0.6120, -0.9213, -0.3720,\n",
       "          0.1944,  0.2549,  0.2056,  0.9949, -0.3452, -0.1319, -0.9423, -0.9890,\n",
       "         -0.0635, -0.8755, -0.0717, -0.6739,  0.5520, -0.1817,  0.0886,  0.3924,\n",
       "         -0.9839, -0.8045,  0.4063, -0.3791,  0.5224, -0.2002,  0.7146,  0.8403,\n",
       "         -0.5794,  0.6525,  0.8934, -0.6888, -0.7708,  0.7749, -0.2771,  0.8685,\n",
       "         -0.7006,  0.9929,  0.6825,  0.5335, -0.9562, -0.3845, -0.8773, -0.4100,\n",
       "         -0.1682, -0.2743,  0.5831,  0.4320,  0.3957,  0.7450, -0.6908,  0.9976,\n",
       "         -0.8594, -0.9585, -0.6457, -0.2748, -0.9892,  0.7752,  0.3780,  0.1494,\n",
       "         -0.4580, -0.6380, -0.9619,  0.8854,  0.1650,  0.9833, -0.3250, -0.9306,\n",
       "         -0.5682, -0.9348, -0.2212, -0.1823,  0.2422, -0.1038, -0.9702,  0.5281,\n",
       "          0.5707,  0.5068, -0.3112,  0.9983,  1.0000,  0.9805,  0.8844,  0.8857,\n",
       "         -0.9983, -0.6119,  1.0000, -0.9533, -1.0000, -0.9239, -0.6746,  0.2397,\n",
       "         -1.0000, -0.1235, -0.0725, -0.9380,  0.1567,  0.9808,  0.9917, -1.0000,\n",
       "          0.9008,  0.9440, -0.5201,  0.8251, -0.4824,  0.9775,  0.4022,  0.5217,\n",
       "         -0.2950,  0.4242, -0.7700, -0.8768, -0.1279, -0.4046,  0.9811,  0.1531,\n",
       "         -0.7156, -0.9465,  0.4008, -0.1710, -0.0315, -0.9656, -0.2111,  0.4102,\n",
       "          0.7231,  0.1516,  0.3509, -0.8021,  0.2328, -0.5455,  0.4320,  0.5908,\n",
       "         -0.9447, -0.6176,  0.2716, -0.3587, -0.2995, -0.9437,  0.9684, -0.4916,\n",
       "          0.6288,  1.0000,  0.4188, -0.8921,  0.5297,  0.2667, -0.2394,  1.0000,\n",
       "          0.7230, -0.9832, -0.4890,  0.6689, -0.5466, -0.4932,  0.9996, -0.2217,\n",
       "         -0.1167,  0.0490,  0.9794, -0.9923,  0.9471, -0.9063, -0.9776,  0.9691,\n",
       "          0.9429, -0.4869, -0.7447,  0.1666, -0.3527,  0.3335, -0.9478,  0.7168,\n",
       "          0.3545, -0.1353,  0.9109, -0.7881, -0.4324,  0.3767, -0.1721,  0.1382,\n",
       "          0.8000,  0.5948, -0.2708,  0.0650, -0.3323, -0.4880, -0.9694,  0.2312,\n",
       "          1.0000, -0.2423,  0.4368, -0.3108, -0.0890, -0.1536,  0.4793,  0.4995,\n",
       "         -0.3567, -0.8845,  0.5553, -0.9447, -0.9911,  0.6794,  0.2624, -0.2930,\n",
       "          1.0000,  0.4425,  0.2925,  0.1576,  0.8787, -0.0432,  0.4729,  0.3847,\n",
       "          0.9754, -0.2582,  0.4169,  0.8205, -0.4750, -0.2669, -0.7019,  0.0698,\n",
       "         -0.9207,  0.1223, -0.9688,  0.9624,  0.6146,  0.3767,  0.1965,  0.4386,\n",
       "          1.0000, -0.5764,  0.5277, -0.1075,  0.8040, -0.9955, -0.7789, -0.4554,\n",
       "         -0.0230, -0.3707, -0.2666,  0.2837, -0.9743,  0.1491,  0.5257, -0.9831,\n",
       "         -0.9945,  0.0898,  0.7444,  0.1715, -0.9190, -0.7186, -0.5349,  0.5420,\n",
       "         -0.2393, -0.9546,  0.3902, -0.2898,  0.4637, -0.2010,  0.4773,  0.2017,\n",
       "          0.7901, -0.2781, -0.0836, -0.1398, -0.7673,  0.7647, -0.7953, -0.7866,\n",
       "         -0.0740,  1.0000, -0.5140,  0.5606,  0.7284,  0.6431, -0.1287,  0.1687,\n",
       "          0.8059,  0.3201, -0.3073, -0.1484, -0.5094, -0.4046,  0.4966,  0.2570,\n",
       "          0.2268,  0.8138,  0.6051,  0.1701,  0.0969,  0.0365,  0.9994, -0.2530,\n",
       "         -0.3062, -0.4775, -0.0299, -0.3373, -0.0562,  1.0000,  0.3479,  0.3388,\n",
       "         -0.9930, -0.6483, -0.9312,  1.0000,  0.7832, -0.8142,  0.6100,  0.6160,\n",
       "         -0.0965,  0.7704, -0.2299, -0.2138,  0.3634,  0.1423,  0.9668, -0.4813,\n",
       "         -0.9708, -0.5214,  0.4172, -0.9622,  0.9989, -0.5598, -0.2905, -0.4267,\n",
       "         -0.4151,  0.2529,  0.1150, -0.9828, -0.2933,  0.1020,  0.9736,  0.2178,\n",
       "         -0.4881, -0.8858,  0.4018,  0.1948, -0.6173, -0.9628,  0.9806, -0.9815,\n",
       "          0.6556,  1.0000,  0.2375, -0.3275,  0.1889, -0.4338,  0.3769, -0.5125,\n",
       "          0.5513, -0.9591, -0.3053, -0.2294,  0.3810, -0.1582, -0.5999,  0.7795,\n",
       "          0.1466, -0.4206, -0.6070, -0.0838,  0.4119,  0.8152, -0.3367, -0.1378,\n",
       "          0.0598, -0.0126, -0.9555, -0.4236, -0.3401, -0.9998,  0.5610, -1.0000,\n",
       "          0.2936, -0.2596, -0.2057,  0.8384,  0.5396,  0.3741, -0.7824, -0.2065,\n",
       "          0.7995,  0.7801, -0.2754, -0.2633, -0.7891,  0.3725, -0.0966,  0.4469,\n",
       "         -0.2534,  0.7190, -0.2084,  1.0000,  0.0228, -0.5043, -0.9706,  0.2989,\n",
       "         -0.3006,  1.0000, -0.8886, -0.9492,  0.5443, -0.5613, -0.8245,  0.3681,\n",
       "         -0.0980, -0.7511, -0.7760,  0.9546,  0.7718, -0.4745,  0.5654, -0.2965,\n",
       "         -0.4881, -0.0135,  0.6650,  0.9903,  0.5524,  0.8905, -0.2984, -0.1938,\n",
       "          0.9551,  0.2327,  0.6217,  0.2077,  1.0000,  0.3327, -0.9078,  0.1122,\n",
       "         -0.9812, -0.1818, -0.9299,  0.3491,  0.2149,  0.8971, -0.2581,  0.9697,\n",
       "         -0.4896,  0.0960, -0.3364,  0.1812,  0.4807, -0.9276, -0.9868, -0.9900,\n",
       "          0.5835, -0.4884,  0.0305,  0.3250,  0.0817,  0.4666,  0.5004, -1.0000,\n",
       "          0.9463,  0.4286,  0.5428,  0.9733,  0.5760,  0.5608,  0.2607, -0.9877,\n",
       "         -0.9753, -0.4241, -0.2934,  0.8090,  0.7190,  0.8551,  0.4776, -0.4918,\n",
       "         -0.4156, -0.0276, -0.8160, -0.9934,  0.3963,  0.0233, -0.9409,  0.9629,\n",
       "         -0.4148, -0.1184,  0.3304, -0.5221,  0.9129,  0.8248,  0.4409,  0.2048,\n",
       "          0.4997,  0.9182,  0.9484,  0.9918, -0.5874,  0.7747, -0.3111,  0.5266,\n",
       "          0.6146, -0.9457,  0.1223,  0.1749, -0.2750,  0.3174, -0.2500, -0.9335,\n",
       "          0.7000, -0.2273,  0.5873, -0.4977,  0.0328, -0.4565, -0.1727, -0.7292,\n",
       "         -0.5687,  0.5303,  0.2038,  0.8994,  0.8242, -0.0432, -0.6686, -0.3017,\n",
       "         -0.2925, -0.9236,  0.9091, -0.1265,  0.0754,  0.1207,  0.0477,  0.8970,\n",
       "         -0.0236, -0.4224, -0.3772, -0.7444,  0.9107, -0.3409, -0.5596, -0.5428,\n",
       "          0.8507,  0.3878,  0.9998, -0.4650, -0.5662, -0.3292, -0.3968,  0.3775,\n",
       "         -0.5127, -1.0000,  0.4716, -0.1930,  0.5168, -0.3026,  0.5821, -0.3216,\n",
       "         -0.9742, -0.3125,  0.2638,  0.1447, -0.5504, -0.5584,  0.4720, -0.1662,\n",
       "          0.9081,  0.9046,  0.2194,  0.5707,  0.5176, -0.1037, -0.7264,  0.9025]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pooler Output**\n",
    "\n",
    "This is meant to represent the whole sequence as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPooler(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We grab the final encoder, the 12ths encoder representation of the CLS token\n",
    "\n",
    "CLS_embedding = response.last_hidden_state[:, 0, :].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the first element (CLS) and the second dimension (the tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3271e-01,  1.5150e-01, -4.4841e-02, -8.4486e-02, -2.5227e-01,\n",
       "          -1.5574e-01,  4.2548e-01,  6.2580e-01, -1.6765e-01, -2.2078e-01,\n",
       "          -4.8964e-02, -1.6687e-01,  3.7025e-01,  5.4881e-01,  5.2734e-02,\n",
       "          -1.8367e-01, -1.0926e-01,  6.5831e-01,  4.5373e-01, -1.1532e-01,\n",
       "           2.0391e-01, -4.1622e-01, -4.4383e-02, -1.0411e-01,  2.5143e-01,\n",
       "          -3.3175e-01, -6.4523e-03,  2.4058e-01,  2.1243e-01, -7.7977e-02,\n",
       "          -1.4318e-01,  2.5448e-01, -2.2074e-02,  9.2747e-02,  9.0562e-02,\n",
       "          -1.2345e-01,  3.4437e-01, -2.0981e-01,  1.8461e-01,  4.8943e-01,\n",
       "           6.5386e-02,  5.8490e-02,  2.9532e-01, -2.0740e-01,  4.4421e-02,\n",
       "          -8.8770e-01, -2.2506e+00,  6.8181e-03,  5.4260e-02, -3.1604e-01,\n",
       "           3.5553e-01,  1.1498e-02,  3.7963e-01,  2.6510e-01,  1.2690e-01,\n",
       "           5.0096e-01, -6.3212e-01,  7.9531e-01,  1.0351e-01,  4.1282e-01,\n",
       "           3.9759e-01, -2.2472e-01, -4.5974e-03,  5.6817e-02, -1.9067e-01,\n",
       "           4.2596e-01, -3.6377e-01,  4.7902e-02,  6.6697e-02,  4.8324e-01,\n",
       "          -1.7540e-01, -1.1834e-01,  4.7304e-01,  2.8369e-01, -1.9196e-01,\n",
       "          -9.1711e-02, -2.6027e-01, -2.2536e-01, -3.0300e-01, -4.0514e-01,\n",
       "           2.8522e-01,  1.9201e-01,  1.9185e-01,  4.0459e-01,  5.4100e-02,\n",
       "           3.8027e-01, -6.4408e-02, -2.2625e-01,  3.7971e-01,  4.6397e-01,\n",
       "          -1.4084e-01,  4.1479e-02,  4.9752e-02,  3.9235e-01,  2.8778e-01,\n",
       "          -1.9220e-02, -3.5515e-01,  2.7780e-01,  1.4806e-01,  1.7618e-01,\n",
       "           2.4188e-01,  2.9370e-01,  3.9465e-01, -5.0245e-01,  5.3768e-02,\n",
       "          -1.2460e-01,  3.8871e-02, -2.1018e-01,  4.2310e-01, -2.9743e+00,\n",
       "           1.9502e-02,  3.7025e-02, -8.8536e-02, -3.2845e-03, -1.9552e-01,\n",
       "           5.7461e-01,  4.0351e-01, -1.7098e-01, -1.1824e-01,  1.8610e-01,\n",
       "          -4.7333e-01,  3.2664e-01,  2.3171e-01, -2.6046e-01,  1.6405e-01,\n",
       "           2.8760e-01, -2.7608e-01, -7.1752e-02,  1.7700e-01,  5.0153e-01,\n",
       "           4.4129e-01,  4.2775e-01, -2.1771e-01, -2.3943e-01,  5.1748e-02,\n",
       "          -1.1557e-01,  5.5351e-01, -1.4321e-01, -2.2027e-01, -5.3599e-02,\n",
       "          -3.7438e-01,  1.4565e-01, -3.0061e+00,  5.1006e-01,  5.5827e-01,\n",
       "           1.2353e-01, -2.9038e-01, -9.4256e-02,  2.8750e-01, -1.0516e-02,\n",
       "           1.5370e-01, -2.0173e-01, -2.7907e-01,  2.2102e-01, -4.1045e-01,\n",
       "          -3.3361e-01, -3.8117e-01, -4.8255e-02,  2.8168e-01,  5.2372e-01,\n",
       "           2.4989e-02, -2.7008e-01,  3.9265e-01, -1.2200e-01, -5.8911e-01,\n",
       "           3.2209e-03,  6.6389e-01,  2.8384e-01, -6.1057e-02,  5.6643e-02,\n",
       "          -1.4621e-01,  1.3663e-01,  2.9812e-01, -1.1022e-01,  2.7444e-01,\n",
       "          -2.3190e-01, -5.4661e-02,  1.6429e-01, -2.3750e-01, -1.8433e-01,\n",
       "          -2.7287e-01,  1.0625e-01, -1.2663e-01,  2.0417e-01,  3.2259e-01,\n",
       "          -1.3488e-01,  1.1290e-01,  1.5272e-01, -1.6448e-01,  1.2574e-01,\n",
       "           3.7891e-01, -1.7376e-02, -5.0127e-02,  1.7129e-01,  4.4656e-01,\n",
       "           6.3907e-02,  2.4988e-01, -7.3124e-01,  1.3210e-01,  1.4171e-01,\n",
       "          -3.2099e-02, -8.7162e-02, -2.8115e-01,  2.4617e-01, -3.4744e-02,\n",
       "           3.7981e+00,  1.2910e-01, -1.0822e-01,  2.3164e-01,  2.0670e-01,\n",
       "          -1.5355e-01,  1.7841e-02,  2.7482e-01, -1.5681e-01, -3.4822e-02,\n",
       "          -3.1938e-01,  3.5485e-01, -8.1459e-02, -2.8530e-01,  1.3362e-01,\n",
       "           9.1637e-02,  4.9123e-01, -2.7023e-01,  3.4055e-01,  3.9455e-01,\n",
       "           2.1837e-02,  1.0550e-01, -1.0088e-01, -3.7485e-01, -9.1229e-01,\n",
       "          -1.1593e-01, -2.9538e-01, -3.8382e-01,  4.2311e-01, -4.1916e-01,\n",
       "           2.9685e-02,  2.0561e-01, -6.5511e-01,  3.7550e-01,  2.1909e-01,\n",
       "           2.4016e-01, -8.6850e-02,  2.4503e-01,  1.2234e-01, -2.8976e-01,\n",
       "           3.5977e-01, -4.7195e-02, -2.7273e-01,  1.5952e-01, -9.7662e-03,\n",
       "           4.3380e-01, -2.6073e-01,  2.7756e-02, -1.4050e-01, -9.9696e-02,\n",
       "          -1.5392e-01,  1.1400e-01,  3.4712e-01, -3.6286e-01,  2.4388e-01,\n",
       "          -4.6852e-01, -2.5083e-01,  3.5077e-01,  1.5889e-02, -2.5785e-01,\n",
       "          -3.1642e-01, -3.2626e-02, -3.6074e-01,  8.0835e-02,  6.2261e-02,\n",
       "          -1.2571e-02, -8.2197e-02, -2.0882e-01, -4.1243e+00,  2.4215e-01,\n",
       "           6.5563e-02, -4.7075e-02,  2.7986e-01,  9.2027e-02,  1.2109e-01,\n",
       "           6.6498e-01,  2.1492e-01, -6.0423e-01,  3.1948e-01,  8.6356e-02,\n",
       "          -3.0297e-01,  7.4286e-02, -1.6794e-01,  2.0143e-01, -7.9770e-02,\n",
       "          -5.9309e-02,  7.1111e-02,  1.6565e-01,  2.7854e-01,  3.9549e-01,\n",
       "          -1.5740e-01,  2.3214e-01, -6.2773e-02,  9.5485e-02, -3.6202e-01,\n",
       "          -1.0392e-01,  1.5567e-01, -2.2148e-01, -8.8502e-03,  1.2735e-01,\n",
       "           5.4708e-02,  1.2296e-01, -2.5543e-03, -2.7640e+00,  2.0462e-01,\n",
       "          -1.7033e-01, -2.7924e-01, -7.9948e-02, -6.8168e-02,  6.3198e-01,\n",
       "          -1.0430e-01, -2.7858e-01,  8.9017e-02, -2.0731e-01,  3.2661e-01,\n",
       "          -6.8196e-02,  3.3931e-01,  2.2884e-01,  4.1722e-02,  3.2533e-01,\n",
       "           2.0262e-01,  5.6677e-02,  4.8391e-02, -2.4207e-01,  3.3559e-01,\n",
       "          -7.6155e-02, -1.6666e-01,  2.6503e-02,  1.1626e-01, -2.4110e-02,\n",
       "          -3.1461e-01, -5.2043e-01,  2.7772e-01, -1.0173e-01,  9.0130e-02,\n",
       "           2.3197e-01, -3.0805e-01, -4.8766e-01, -2.6091e-01,  6.8948e-02,\n",
       "           1.4990e-01,  5.4066e-01,  2.9442e-01, -6.4465e-02,  3.2198e-01,\n",
       "          -2.4334e-01,  4.9293e-01,  4.0538e-01, -5.0312e-02, -3.4797e-01,\n",
       "          -3.7703e-01,  4.0331e-01,  2.8818e-01,  5.5191e-02, -1.8081e-01,\n",
       "           1.1092e+00, -3.6286e-01, -2.3143e-01, -2.4927e-01,  5.3777e-01,\n",
       "          -4.7891e-02, -1.7562e-01,  7.8011e-03,  6.5151e-01, -1.0074e-01,\n",
       "           2.0054e-01, -1.7141e-02,  3.6204e-03, -2.9117e-01,  6.2743e-01,\n",
       "          -7.1717e-01, -3.2403e-01,  4.4017e-01, -2.6473e-01,  3.5107e-02,\n",
       "           2.0456e-03, -8.7687e-01, -7.8443e-02,  3.1520e-01, -3.5067e-01,\n",
       "          -9.5559e-02,  3.7142e-01, -2.8034e-01, -4.8419e-02, -1.7665e-01,\n",
       "          -1.2086e-01,  4.6752e-01, -2.3944e-01, -2.4077e-01, -5.2498e-02,\n",
       "           1.5029e-01, -3.9954e-01,  2.2462e-01, -1.0264e-01,  3.0041e-01,\n",
       "           8.7730e-02,  9.3927e-02, -3.2150e-02,  1.2056e-01,  5.1987e-02,\n",
       "          -8.1530e-01, -3.6623e-02, -5.4405e-01, -1.9435e-01, -2.4741e-01,\n",
       "          -4.6362e-01,  4.0258e-01, -1.5637e-01, -4.4997e-01,  4.3708e-02,\n",
       "           2.9951e-01, -1.1724e-01,  3.6855e-01, -1.4638e-01,  6.3745e-02,\n",
       "           1.9394e-02,  1.7863e-01,  1.1453e+00, -3.0523e-01,  2.6115e-01,\n",
       "           7.5353e-01, -2.3605e-01,  2.7187e-01,  2.7742e-01,  2.2828e-01,\n",
       "           2.1079e-01, -3.7611e-01, -5.4407e-01, -2.0564e-01,  1.1459e-01,\n",
       "          -5.8399e-01, -6.2641e-01, -1.2705e-02,  4.1699e-02, -4.5128e-01,\n",
       "          -3.3281e-01, -7.8684e-01, -7.4439e-02, -1.2194e-01, -5.4343e-01,\n",
       "          -6.4499e-02,  3.8756e-01,  3.0978e-02,  3.3590e-02, -1.2790e-02,\n",
       "          -2.3473e-01,  6.0331e-01, -8.7469e-02,  2.4502e-01, -1.6894e-01,\n",
       "          -8.4193e-02, -1.8097e-01,  3.0655e-01, -1.8486e-01, -3.5678e-01,\n",
       "           2.1142e-01, -2.5320e-01, -2.4978e-02, -3.1561e-01, -1.9696e-01,\n",
       "          -1.6572e-01,  6.0212e-02,  2.3850e-01, -1.6153e-01, -1.4312e-01,\n",
       "          -1.3164e+00,  1.4156e-01,  1.3825e-01, -1.4349e-01,  2.1656e-01,\n",
       "          -2.3591e-01, -2.4127e-01,  5.6333e-01, -1.9460e-01,  3.0315e-01,\n",
       "          -1.0494e-01, -2.5616e-01, -2.3852e-01,  1.4533e-01,  1.4027e-01,\n",
       "           2.0537e-01,  1.9542e-01, -3.0467e-01, -4.4110e-02,  4.7004e-02,\n",
       "          -2.5079e-01,  4.3413e-01,  8.2384e-02, -4.5616e-03, -1.6884e-01,\n",
       "          -2.4162e-01, -3.1745e-02,  5.1825e-01, -3.4510e-02,  4.4123e-01,\n",
       "           6.1829e-02, -1.9817e-01, -5.1227e-01, -3.1855e-01,  2.6614e-01,\n",
       "           2.1126e-01,  3.8337e-01, -4.4167e-02,  1.1224e-02,  1.9248e-01,\n",
       "          -8.6594e-02,  4.5655e-01,  1.4418e-01, -4.5307e-01,  5.5336e-01,\n",
       "           3.9465e-01, -7.5446e-02,  2.7631e-01, -1.7485e-02, -7.7119e-01,\n",
       "           9.8931e-02, -8.4585e-03,  8.8241e-02, -1.6717e-01,  1.7343e-01,\n",
       "          -7.0189e-02,  4.6361e-02, -1.0772e-01, -3.3707e-01,  6.4598e-02,\n",
       "           7.5915e-02, -6.3854e-01, -1.5701e-01,  2.2976e-01, -5.7939e-01,\n",
       "          -2.9358e-01,  2.1733e-01, -1.7873e-01,  1.3760e-03,  2.6652e-01,\n",
       "           5.2109e-01,  6.7167e-02, -2.3424e-01,  3.2958e-02, -2.9696e-01,\n",
       "          -1.0217e-01, -1.0653e-01,  2.6575e-01, -1.3395e-02, -3.1416e-01,\n",
       "           1.1417e-01, -6.2598e-01, -3.9436e-01,  3.9622e-01, -4.2629e-01,\n",
       "           5.2567e-02, -2.6184e-01, -9.7967e-02, -5.9486e-02,  1.7955e-01,\n",
       "          -2.3958e-01, -3.2173e-01,  2.2700e-01,  1.6213e-01, -7.5971e-02,\n",
       "           4.8483e-02,  1.4512e-01,  6.8626e-02, -2.9257e-02,  8.7811e-02,\n",
       "          -9.2341e-02,  5.0805e-01,  4.7803e-01,  1.6944e-01,  5.0989e-02,\n",
       "           4.4003e-01,  3.9077e-01,  3.2461e-01, -5.3626e-01, -5.5734e-01,\n",
       "           8.6009e-02,  1.2305e-01, -3.3298e-01, -1.8356e-01,  5.3951e-02,\n",
       "           1.0895e-01,  2.4193e-01, -2.2119e-01,  2.3096e+00,  3.7287e-01,\n",
       "           3.2175e-01,  6.2478e-02,  1.4194e-01,  1.2400e-02, -3.6991e-01,\n",
       "           2.2460e-01, -1.0639e-01,  3.4922e-01,  2.8070e-01,  1.3629e-01,\n",
       "          -2.9208e-01,  2.8242e-01,  6.2653e-01,  3.5009e-01,  2.2607e-01,\n",
       "          -3.1114e-01, -3.1567e-01, -8.5231e-02, -3.9861e-01,  4.9909e-01,\n",
       "           2.4460e-02, -2.7680e-01,  1.2404e-01,  8.4534e-02,  1.7443e-01,\n",
       "           1.3809e-04, -1.5442e-02,  2.6792e-01, -2.9142e-01,  1.6812e-01,\n",
       "           1.7693e-01,  5.5260e-01, -1.4287e-02,  1.0541e-01,  1.7468e-01,\n",
       "          -2.4426e-01, -7.5408e-02,  1.0573e-01, -3.4213e-02, -9.1429e-01,\n",
       "           4.5514e-01,  2.7479e-02, -1.2022e-01,  4.2621e-01, -3.8579e-02,\n",
       "           6.0850e-02,  1.9165e-01,  2.2973e-01, -3.5223e-01, -2.4133e-01,\n",
       "          -1.6484e-01, -1.4955e-01, -5.8842e-02, -2.4653e-01, -7.2820e-02,\n",
       "          -2.9235e-01, -9.5051e-02,  2.3113e-01,  2.2910e-01, -3.3116e-02,\n",
       "           3.9613e-01, -4.6181e-02,  4.1119e-01, -2.7936e-01, -3.3864e-01,\n",
       "           6.1473e-01,  1.8150e-01, -2.7822e-01,  8.8118e-02,  3.7072e-01,\n",
       "           5.6828e-01, -5.8368e-01,  1.6617e-01, -1.9917e-01,  2.3742e-01,\n",
       "          -1.9109e-01, -2.3390e-02, -3.1606e+00, -3.8811e-02,  2.0464e-01,\n",
       "           1.7865e-01,  2.2959e-02,  7.8355e-01, -1.8946e-02,  2.3248e-01,\n",
       "          -2.6050e-02, -4.8583e-01,  4.9811e-02,  2.1655e-01,  2.2684e-01,\n",
       "           2.5147e-02,  2.2905e-01,  1.3834e-01,  3.5491e-01, -6.8223e-01,\n",
       "          -3.8659e-01, -1.8242e-02,  1.1925e-01, -2.4854e-02,  8.9465e-02,\n",
       "          -2.5524e-01, -2.9678e-01,  8.0817e-02, -3.3148e-02, -3.8346e-01,\n",
       "          -3.5161e-01,  3.1135e-02,  1.1605e-02,  3.7174e-01,  3.1142e-01,\n",
       "           6.4175e-03, -1.3988e-02, -8.8407e-02, -6.1732e-02, -1.3230e-01,\n",
       "           3.5776e-01, -6.9503e-02,  4.2132e-02,  3.6793e-01, -2.8130e-01,\n",
       "           8.6114e-02, -3.4317e-01,  1.4835e-01,  5.5059e-02, -2.8975e-01,\n",
       "           4.5604e-01, -3.5256e-01,  2.4709e-01,  3.3851e-02,  1.0709e-01,\n",
       "           2.1092e-01,  1.8611e-01,  3.4126e-02,  3.1324e-01,  3.4413e-01,\n",
       "          -6.1849e-03, -3.0519e-01, -1.3165e-01,  2.8448e-01,  9.3675e-03,\n",
       "           8.9184e-02,  1.4214e-01, -1.8068e-01,  6.4513e-03, -8.5885e-02,\n",
       "           7.9977e-02,  1.9017e-02, -2.4783e-01, -1.4666e-01,  1.3989e-01,\n",
       "           3.3144e-01,  2.1827e-01,  2.4801e-01,  3.9463e-01,  2.1714e-01,\n",
       "           2.0130e-02,  2.7346e-01, -1.5399e-01, -2.2640e-02, -5.6779e-01,\n",
       "           2.6773e-01,  2.6846e-01, -8.0467e+00, -4.8366e-02, -6.3572e-01,\n",
       "           1.1081e-01,  6.4151e-02, -3.5784e-01, -9.4167e-02, -1.3992e-01,\n",
       "           1.3511e-01,  2.3833e-01,  1.0321e-01, -1.4531e-01,  8.7714e-02,\n",
       "          -5.1924e-01,  4.1946e-01,  2.9482e-01]]],\n",
       "       grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLS_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLS_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One batch, One token, 768 dimensions in the final representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pooler(CLS_embedding).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the vector representation of the entire sequence at large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model pooler is a feed forward network with a hyperbolic activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One batch, 768 dimensional vector of the final representation of the sequence at large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the embedding for CLS through the pooler gives the same output\n",
    "# as the pooler output\n",
    "\n",
    "(model.pooler(CLS_embedding) == response.pooler_output).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
