{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Introducction to transfer learning***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranfer learning is where a model is trained for as taks and then reused as the starting point for a model for a second task.\n",
    "\n",
    "Approach:\n",
    "\n",
    "- Select source model: Choose a pre-trained source model generally from a repository of models\n",
    "- Reuse and train model: The pre-trained model is then the starting point for a second related taks and trained on data pertaining to the second task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfer learning on NLP**\n",
    "\n",
    "A model is pre-trained on an unlabeled text corpora on an unsupervised taks that generally doesn't have a \"useful\" objective, it is just meant to learn language/context in general\n",
    "\n",
    "The model is then fine-tuned on a specific NLP \"downstream\" task using a labeled dataset that is usually quite small in comparison\n",
    "\n",
    "If we simply trained on the downstream smaller dataset without pre-training, we would never be able to achieve the same state of the art results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approaches to fine tuning**\n",
    "\n",
    "- 1: Update the whole model on labeled data + any additional layers added on top\n",
    "- 2: Freeze a subset of the model\n",
    "- 3: Freeze the whole model an only train the additional layers added on top"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
