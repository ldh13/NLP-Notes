{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tokenize + Embedding layer***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sentence:\n",
    "\n",
    "\"Another beautiful day\"\n",
    "\n",
    "To tokenize this, we split it into a list of tokens in our vocabulary over 30000 tokens.\n",
    "\n",
    "We also add two special tokens CLS at the beginning and SEP at the end.\n",
    "\n",
    "CLS is meant to represent the entire sequence and SEP is meant to represent separation between sentences.\n",
    "\n",
    "[\"CLS\", \"another\", \"beautiful\", \"day\", \"SEP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What about new words**\n",
    "\n",
    "In \"Sinan loves a beautiful day\", BERT does not recognize \"Sinan\" and it becomes:\n",
    "\n",
    "[\"CLS\", \"sin\", \"##an\", \"loves\", \"a\", \"beautiful\", \"day\", \"SEP\"]\n",
    "\n",
    "Where \"##\" indicates a subword\n",
    "\n",
    "BERT's tokenizer is great at handling tokens that are OOV (Out of Vocabulary) by breaking them up into smaller chunks of known tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Freezing the model**\n",
    "\n",
    "If we pass BERT information on a specific domain where there are many OOVs we need to be careful with freezing since that will prevent BERT from learning the specific usage of those tokens in the greater domain's context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on tokenization**\n",
    "\n",
    "BERT has a maximum sequence length of 512 tokens. This was implemented for the sake of efficiency.\n",
    "\n",
    "Any sequence less than 512 tokens will be padded to reach 512 and if over 512 the model may error out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncases VS Cased tokenization**\n",
    "\n",
    "- Uncased: Removes accents and lower cases the input\n",
    "- Cased: Does nothing to the input\n",
    "\n",
    "Uncased tokenization is usually best for most situations since case doesn't usually contribute to context.\n",
    "\n",
    "Cased tokenizations works really well in cases where Named Entity Recognition is important.\n",
    "\n",
    "Note that his has little to do with the BERT architecture but with tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Words with context**\n",
    "\n",
    "Consider the following sentences:\n",
    "\n",
    "\"I love my pet Python\"\n",
    "\n",
    "\"I love coding in Python\"\n",
    "\n",
    "The token \"Python\" will end up with a different vector representation for each sentence, this is because of the surrounding words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nico-\\NLPs\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the BERT model\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of BERT's vocabulary is: 30522\n"
     ]
    }
   ],
   "source": [
    "# Checking the vocabulary length\n",
    "\n",
    "print(f\"The length of BERT's vocabulary is: {len(tokenizer.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1037, 3722, 6251, 102]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing a simple sequence\n",
    "\n",
    "text = \"A simple sentence\"\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how we get the CLS (101) and the SEP (102) tokens and one token for each word, meaning they were already in BERT's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 14941, 15987, 2078, 2003, 2025, 2613, 2171, 102]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking a token not in BERT's vocabulary\n",
    "\n",
    "text1 = \"Lochlainn is not real name\"\n",
    "tokens1 = tokenizer.encode(text1)\n",
    "\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly BERT had never seen such a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] a simple sentence [SEP]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconstructing the original sentence\n",
    "\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293, 2009, 2061, 2521, 999, 2016, 2001, 2157, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# Looking at a more complex sequence\n",
    "\n",
    "text2 = \"My friend told me about this class and I love it so far! She was right.\"\n",
    "tokens2 = tokenizer.encode(text2)\n",
    "\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My friend told me about this class and I love it so far! She was right.. Num tokens: 20\n",
      "Token: 101, subword: [ C L S ]\n",
      "Token: 2026, subword: m y\n",
      "Token: 2767, subword: f r i e n d\n",
      "Token: 2409, subword: t o l d\n",
      "Token: 2033, subword: m e\n",
      "Token: 2055, subword: a b o u t\n",
      "Token: 2023, subword: t h i s\n",
      "Token: 2465, subword: c l a s s\n",
      "Token: 1998, subword: a n d\n",
      "Token: 1045, subword: i\n",
      "Token: 2293, subword: l o v e\n",
      "Token: 2009, subword: i t\n",
      "Token: 2061, subword: s o\n",
      "Token: 2521, subword: f a r\n",
      "Token: 999, subword: !\n",
      "Token: 2016, subword: s h e\n",
      "Token: 2001, subword: w a s\n",
      "Token: 2157, subword: r i g h t\n",
      "Token: 1012, subword: .\n",
      "Token: 102, subword: [ S E P ]\n"
     ]
    }
   ],
   "source": [
    "# Looking at each token in more detail\n",
    "\n",
    "print(f'Text: {text2}. Num tokens: {len(tokens2)}')\n",
    "\n",
    "for t in tokens2:\n",
    "    print(f'Token: {t}, subword: {tokenizer.decode(t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sinan is not in the vocabulary\n",
    "\n",
    "'Sinan' in tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 8254, 2319, 7459, 1037, 3376, 2154, 102]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at tokenization with OOV\n",
    "\n",
    "text_unkown_words = \"Sinan loves a beautiful day\"\n",
    "tokens_unknown = tokenizer.encode(text_unkown_words)\n",
    "\n",
    "tokens_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 101, word/subword: [ C L S ]\n",
      "Token: 8254, word/subword: s i n\n",
      "Token: 2319, word/subword: # # a n\n",
      "Token: 7459, word/subword: l o v e s\n",
      "Token: 1037, word/subword: a\n",
      "Token: 3376, word/subword: b e a u t i f u l\n",
      "Token: 2154, word/subword: d a y\n",
      "Token: 102, word/subword: [ S E P ]\n"
     ]
    }
   ],
   "source": [
    "# Looking at each individual token in the sequence\n",
    "\n",
    "for t in tokens_unknown:\n",
    "    print(f'Token: {t}, word/subword: {tokenizer.decode(t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293, 2009, 2061, 2521, 999, 2016, 2001, 2157, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using encode plus\n",
    "# Will give us the token ids and an attention mass\n",
    "# It is a sequence of 0s and 1s, 1 if the token should be taken into consideration\n",
    "# for attention computations and 0 otherwise\n",
    "\n",
    "text = 'My friend told me about this class and I love it so far! She was right.'\n",
    "tokens = tokenizer.encode_plus(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293, 2009, 2061, 2521, 999, 2016, 2001, 2157, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the tokenizer directly is the same as using encode_plus\n",
    "\n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token type id shows that all tokens in the sequence come from the first (and only) sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python example\n",
    "\n",
    "text0 = 'I love my pet Python'\n",
    "text1 = 'I love coding in Python'\n",
    "\n",
    "python_pet = tokenizer.encode(text0)\n",
    "python_coding = tokenizer.encode(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 2293, 2026, 9004, 18750, 102]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 2293, 16861, 1999, 18750, 102]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how \"Python\" is the same token, now we will pass these as tensors into the BERT model and compute the cosine similarity after we have added context to the vector representation of each of the tokens on the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding context to pet sequence\n",
    "\n",
    "python_pet_embedding = model(torch.tensor(python_pet).unsqueeze(0))\\\n",
    "[0][:, 5, :].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context to coding sequence\n",
    "\n",
    "python_coding_embedding = model(torch.tensor(python_coding).unsqueeze(0))\\\n",
    "[0][:, 5, :].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will compare the word snake to each of these two vector\n",
    "# representations of the word Python\n",
    "# And the same with the word programming\n",
    "\n",
    "snake_embedding = model(torch.tensor(tokenizer.encode('snake')).unsqueeze(0))\\\n",
    "[0][:, 1, :].detach().numpy()\n",
    "\n",
    "programming_embedding = model(torch.tensor(tokenizer.encode('programming'))\n",
    "                              .unsqueeze(0))[0][:, 1, :].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5843479]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity between the word Python in a sentence about coding and snake\n",
    "\n",
    "cosine_similarity(python_coding_embedding, snake_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6928655]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity between the word Python in a sentence about pets and snake\n",
    "\n",
    "cosine_similarity(python_pet_embedding, snake_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5614743]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity between programming and python from coding sentence\n",
    "\n",
    "cosine_similarity(python_coding_embedding, programming_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49864346]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity between python from pet sequence and programming\n",
    "\n",
    "cosine_similarity(python_pet_embedding, programming_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
