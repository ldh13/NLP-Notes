{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Flavors of BERT***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT Architectures**\n",
    "\n",
    "BERT inspired derivative architectures, each with their own specialties/drawbacks\n",
    "\n",
    "- RoBERTa\n",
    "- DistillBERT\n",
    "- ALBERT\n",
    "\n",
    "Each flavor attempts to enhance BERT by altering its architecture and/or how it was pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RoBERTa**\n",
    "\n",
    "- Authors claim BERT was under trained\n",
    "- 10x the training data, from 16gb to 160gb\n",
    "- 15% more parameters\n",
    "- Removed the next sentence prediction task, they claim that the taks is not useful\n",
    "- Dynamic Masking Pattern -> 4x the masking tasks to learn from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DistillBERT**\n",
    "\n",
    "- Distillation is a technique to train a student model to replicate a master model (BERT would make a prediction and compute the losses and DistillBERT would use those loses in its back propagation)\n",
    "- 40% fewer parameters and 60% faster in training and prediction while having 97% of BERT's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALBERT**\n",
    "\n",
    "- Optimize model performance /number of parameters (90% fewer parameters)\n",
    "- Factorizing embedding parameterization architecture, factorizing tokens to make them much smaller\n",
    "- Cross-layer parameter sharing architecture, share parameters across layers\n",
    "- Next sentence prediction task became the Sentence order prediction task (the theory is that NSP and MLM are too similar of a task while SOP is harder)\n",
    "Take two consecutive parts from the same document and swap the order to use it as a negative example\n",
    "\n",
    "e.g. \"I completed highschool\" \"Then I joined undergrad\" and ALBERT has to learn the correct order, since in next sentence prediction we take sentences from two different documents we can't for sure say they wouldn't come one after the other while swapping the sentence order gave a more robust sample base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nico-\\NLPs\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "If you don't *** at the sign, you will get a ticket\n",
      "Token: look, Score: 0.47%\n",
      "Token: stop, Score: 0.43%\n",
      "Token: glance, Score: 0.01%\n",
      "Token: wait, Score: 0.01%\n",
      "Token: turn, Score: 0.01%\n"
     ]
    }
   ],
   "source": [
    "# Instantiating model for masking\n",
    "\n",
    "nlp = pipeline('fill-mask', model='bert-base-cased')\n",
    "\n",
    "preds = nlp(f\"If you don't {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print(\"\\n\\nIf you don't *** at the sign, you will get a ticket\")\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token: {p['token_str']}, Score: {p['score']:,.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 143kB/s]\n",
      "c:\\Users\\nico-\\NLPs\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nico-\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading model.safetensors: 100%|██████████| 499M/499M [03:55<00:00, 2.12MB/s] \n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 1.09MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.40MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 1.90MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "If you don't *** at the sign, you will get a ticket\n",
      "Token:  look, Score: 0.44%\n",
      "Token:  stop, Score: 0.41%\n",
      "Token:  stay, Score: 0.03%\n",
      "Token:  stand, Score: 0.02%\n",
      "Token:  wave, Score: 0.01%\n"
     ]
    }
   ],
   "source": [
    "# Same task using RoBERTa\n",
    "\n",
    "nlp = pipeline('fill-mask', model='roberta-base')\n",
    "\n",
    "preds = nlp(f\"If you don't {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print()\n",
    "print(type(nlp.model))\n",
    "\n",
    "print(\"\\n\\nIf you don't *** at the sign, you will get a ticket\")\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token: {p['token_str']}, Score: {p['score']:,.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these models are doing the same process, using attention and CLS and SEP tokens, the difference comes down to the architecture and how the models were pretrained."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
