{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Encoder/Decoder Architectures***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformers**\n",
    "\n",
    "The transformer is a current state of the art NLP model.\n",
    "It relies almost entirely on self-attention to model the relationship between tokens in a sentence rather than relying on recursion like RNNs and LSTMs do.\n",
    "\n",
    "The transformer consists of an encoder and a decoder.\n",
    "The encoder takes in our input and outputs a matrix representation of that input. The decoder takes in that representation and iteratively generates output\n",
    "\n",
    "e.g. ENCODER -> \"How are you\" -> MATRIX REPRESENTATION -> DECODER ->\"Nasilin?\"  (English to turkish translation using a transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Encoder**\n",
    "\n",
    "The encoder is actually an encoder stack with multiple encoders.\n",
    "The original input is passed into encoder 1 which creates a representation which is then passed on to econder 2 up until encoder N which itself outputs a representation feed to the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Decoder**\n",
    "\n",
    "The decoder is actually a decoder stack with multiple decoders, and it is decoder N which actually predicts the words that will be outputted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross attention**\n",
    "\n",
    "The input embeddings from the encoders feeds these embeddings to the decoders using cross attention, a special part of attention that takes part of the attention from the encoder and decoder which is the way the model correlates them together and relates what is being input and what is output.\n",
    "\n",
    "This is how we get probabilities of what it should be outputted next."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
