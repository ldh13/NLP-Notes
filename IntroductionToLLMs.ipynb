{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Large Language Models***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brief history of NLPs**\n",
    "\n",
    "In 2001 appeared the first neural language model to predict future words in a sentence using a feed-forward neural network.\n",
    "\n",
    "2013 there are two methods of solving the Word2Vec problem, creating word embeddings at scale that retain semantic meaning in a vector space.\n",
    "\n",
    "These two methods were used to learn semantic and association meaning in a vector space.\n",
    "\n",
    "Learning from large-scale corpora was already causing bias issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BIas**\n",
    "\n",
    "- Direct Bias: terms like football are inherently closer to male words where nurse is closer to female words\n",
    "\n",
    "- Indirect Bias: more nuanced correlations in the learned embeddings leading to teacher being closer to volleyball than football due to larger female associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNNs and CNNs**\n",
    "\n",
    "Recurrent neural networks and convolutional neural networks.\n",
    "\n",
    "Sequence to sequence models: an encoder reads a variable length input sequence (e.g. english words) and a decoder produces a variable length sequence output (e.g. translated French words) using a fixed-length hidden state in the middle\n",
    "\n",
    "The problem with compressing a sequence into a fixed-length vector makes it hard to remember long-term information. Attention allows the decoder access to the encoder hidden states, which are provided as additional input to the decoder.\n",
    "\n",
    "Instead of a fixed length vector being outputted from the encoder to the decoder, the decoder could look at the hidden state of every single token along the way, basically allowing it to pay attention to intermediary tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention is all you need**\n",
    "\n",
    "Attention could be the primary mechanism, powering a LLM to replace slow recurrent connections.\n",
    "\n",
    "By 2018 pre-trained LLMs showed much better performances than CNNs and RNNs in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paying attention with attention**\n",
    "\n",
    "The transformer model is based primarly based on the idea of attention, which is a family of mechanisms designed to \"attend\" to parts of a sequence in the context of another sequence usually for the purpose of some prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention example**\n",
    "\n",
    "The image captioning problem in which we try to make a model that can create a caption for an image allowed us to see which parts of the image the model was \"attending\" to once we saw the output caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-attention**\n",
    "\n",
    "Consider the following phrase:\n",
    "\n",
    "\"My friend told me about this class and I love it so far! She was right.\"\n",
    "\n",
    "- The pronoun \"I\" refers to the antecedent \"me\"\n",
    "- The pronoun \"She\" refers to the antecedent \"friend\"\n",
    "- The pronoun \"I\" also refers to the fact that I \"love\" the class\n",
    "\n",
    "This is the challenge to teach an architecture all these gramatical rules when we feed it a simple sentence such as the one above.\n",
    "\n",
    "With self-attention, we alter the idea of attention to relate to each word in the phrase with every other word in the phrase to teach the model the relationship between them.\n",
    "\n",
    "It teaches the model to relate the sequence with itself.\n",
    "\n",
    "A bert model would signal a relationship between \"I\" and \"me\" and also between \"She\" and \"friend\", showing it understans the semantic and gramatical relationships between the words in the sequence."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
