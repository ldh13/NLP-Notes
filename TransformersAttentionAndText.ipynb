{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tansformers, Attention and Text***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intro to Transformers**\n",
    "\n",
    "Back in the day RNNs were used mainly for language modelling or NLP taks while CNNs were mostly used for computer vision, since the transformer revolutionized the industry it has started to replace RNNs for NLP tasks and how a type of transformers has now started to take over computer vision tasks as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaled Dot Product Attention**\n",
    "\n",
    "\"Time to get into the meat of the Math\"\n",
    "\n",
    "Standard attention is calculated using three matrices, query, key and value matrices.\n",
    "\n",
    "LINEAR ALGEBRA ALERT\n",
    "\n",
    "X = Our tokenized input (Matrix)\n",
    "\n",
    "[\n",
    "    [_, _, _, _], (token 1)\n",
    "    [_, _, _, _]  (token 2)\n",
    "]\n",
    "\n",
    "The four columns of this matrix is the token dimension, meaning each token is given an embedding \n",
    "\n",
    "(Word2Vec embedding that in some way represents the semantic meaning of the token)\n",
    "\n",
    "Each row of the matrix represents a token embedding for each token in the input\n",
    "\n",
    "The input has already gone through BERT's preprocessing (token, segment and position)\n",
    "\n",
    "In this example, we have an input of 2 tokens.\n",
    "\n",
    "The Matrix has shape:\n",
    "\n",
    "number of tokens x token dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know perform three linear transformations on the Matrix X to project it onto three new vector spaces:\n",
    "\n",
    "- The query space\n",
    "- The key space\n",
    "- The value space\n",
    "\n",
    "X x Wq = Q\n",
    "X x Wk = K\n",
    "X x Wv = V\n",
    "\n",
    "We still mantain the two rows, one for each token but the number of columns, i.e. token dimension changes\n",
    "\n",
    "Now each Matrix have an intutive meaning which gives raise to the attention formula.\n",
    "\n",
    "We end up with token dimension and embedding dimension, compressing the information into the resulting matrices Q, K and V.\n",
    "\n",
    "We want to impart meaning to these dimensions to allow a training phase to learn the values for the W matrices to impart the meaning that the architect intended.\n",
    "\n",
    "The query matrix is meant to represent the information we are looking for, the point of the sequence.\n",
    "\n",
    "The key matrix intuitively represents the relevance of each word to the sequence, how important each word is to the overall query\n",
    "\n",
    "The value matrix represents contextless meaning of the input token, V is basically a compressed version of X.\n",
    "Where X was the embedding of the sequence and V is a more compressed version of it. Pre-attention meaning, i.e. what does each word mean on its own before considering the sequence as a whole.\n",
    "\n",
    "Eventually we want to have a context full representation which takes into account the rest of the tokens.\n",
    "\n",
    "All of these matrices contain information regarding individual tokens, query is the information regarding the tokens and the point of the sentence, key embeddings has information on each token as it relates to the query and the V contains the semantic meaning given no context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaled Dot Product Formula**\n",
    "\n",
    "It is the soft-max of the query matrix times the key matrix (transposed) divided by the square root of the embedding dimension.\n",
    "\n",
    "We take the soft-max so that the numbers add up to one and we have a probability distribution and then we multiply the whole thing by V.\n",
    "\n",
    "The last past of multiplying the matrix by V is what transforms V from contextless into a contextfull representation of each token.\n",
    "\n",
    "This forces the tokens to look at each other and scale the context less representation V into context full representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Breaking down the formula**\n",
    "\n",
    "When we multiply Q times K transpose we have number of tokens by number of tokens, this represents the attention score of the tokens.\n",
    "\n",
    "When we think back to the english vs french translation this is the kind of square matrix we calculate here, and we should see bigger numbers where the attention is bigger.\n",
    "\n",
    "Then we divide these attention matrix by the square root of the number of dimensions, these is only done to make the numbers smaller and faster by to compute while not loosing information.\n",
    "\n",
    "Then we take the soft-max to have every row being a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention scores**\n",
    "\n",
    "Smaller values meant vectors were not cloase and therefore less attention, while large values meant vectors were close and therefore more attention.\n",
    "\n",
    "When we take an attention score in relation to a specific word or token we are computing how much each other word and itself correlates with that specific token.\n",
    "\n",
    "Remember these vectors live in different vector spaces, this is why saying the word like is different to the concept of \"I  like cats\", when we calculate query times key transpose we compute a number of how close these vectors are close to each other.\n",
    "\n",
    "This is not semantic similarity but relevance of each word to each other word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Attention Matrix**\n",
    "\n",
    "The rows of the matrix now represent the tokens and each columns would represent how much attention should be payed to it.\n",
    "\n",
    "Now we multiply by the value matrix to scale each value and know how much value to give to each token and end up with a context full representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Soft-Max**\n",
    "\n",
    "It allows us to have a confidence level of how much attention to pay for each token, to put it another way when as a query I wonder why am I saying the work like it should be paying attention to a given word some probability value that we got from the soft-max.\n",
    "\n",
    "When we then multiply by the value of each token (this compressed semantic meaning of each word in the matrix value (each word value in a vaccum)), we give a semantic meaning to each word and should have a particular importance based on the attention scores we got in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The end result**\n",
    "\n",
    "We end up with a matrix where each row represents a token and each column represents a context full dimension of meaning.\n",
    "\n",
    "By taking the attention scores and multiplying by the value matrix we explicitly force those tokens to look at each other and then they are adjusting the value matrix to represent the context-full representation of the relevance to the actual sequence at large."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
