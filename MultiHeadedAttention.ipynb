{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the model know which tokens to pay attention to?**\n",
    "\n",
    "If one attention mechanism will start to learn different patterns and focus on a specific grammar rule more of them.\n",
    "\n",
    "This is where multi-headed self-attention comes in.\n",
    "\n",
    "Multiple smaller attention mechanisms (of smaller embedding dimension) concatenated together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Architecture**\n",
    "\n",
    "Each encoder is comprised of several self-attention mechanisms.\n",
    "\n",
    "They don't actually talk to each other while doing so.\n",
    "\n",
    "In each encoder we apply multi-headed attention:\n",
    "\n",
    "- This is out input sentence, \"Thinking machines\"\n",
    "- We embed each word (or token)\n",
    "- Split into 8 heads, and multiply X (or R, where R is the representation matrix outputted by the preceding encoder) with weight matrices\n",
    "- Calculate attention using the resulting Q, K and V matrices\n",
    "- Concatenate the resulting 2 matrices then multiply with weight matrix Wo to produce the output layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with matrices Z0 to Zn (one for each head) which concatenated are then multiplied by the Wo matrix results in a final Z output matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the iterations will be zooming in on a different component of the overall meaning in the sequence.\n",
    "\n",
    "All this Matrices are concatenated and we end up with a matrix with one row for each token and the columns containing the dimensions of meaning from each head, then we multiply by a final weight W and we get the end matrix Z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use multiheaded attention**\n",
    "\n",
    "- Helps BERT focus on different relationships simultaneously\n",
    "- Helps BERT learn different types of relations between words by transforming our inputs to multiple sub-spaces of Q/K/V\n",
    "\n",
    "e.g. \"Sinan ate the cereal and then he had stomach ache\"\n",
    "\n",
    "- A: The word \"he\" relates to Sinan as a pronoun\n",
    "- B: The word \"he\" also relates to had"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
